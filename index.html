<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding">
  <meta name="keywords" content="ROMA, Omni-multimodal, Streaming, VideoLLM, ACL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ROMA: Real-time Omni-Multimodal Assistant</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  
  <style>
    body { font-family: 'Noto Sans', sans-serif; }
    .title.is-1 { font-family: 'Google Sans', sans-serif; }
    .publication-title { font-size: 2.5rem; font-weight: 700; margin-bottom: 1rem; }
    .publication-authors { font-size: 1.2rem; color: #4a4a4a; margin-bottom: 1.5rem; }
    .publication-links { display: flex; justify-content: center; gap: 10px; margin-top: 1rem; flex-wrap: wrap; }
    .abstract-text { font-size: 1.1rem; line-height: 1.6; text-align: justify; }
    .section-title { margin-top: 3rem; margin-bottom: 1.5rem; }
    .hero-body { padding-bottom: 1rem; }
    .teaser-caption { font-size: 0.9rem; color: #666; margin-top: 0.5rem; }
    .bibtex-box { background-color: #f5f5f5; padding: 15px; border-radius: 5px; overflow-x: auto; font-family: monospace; font-size: 0.9rem; }
    
    /* 视频容器样式 - 16:9 */
    .publication-video {
      position: relative;
      padding-bottom: 56.25%;
      height: 0;
      overflow: hidden;
      max-width: 100%;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      margin-top: 2rem;
    }
    .publication-video iframe {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    /* 图片通用样式 */
    .result-image {
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0,0,0,0.1);
      margin-bottom: 10px;
    }
    
    /* 分隔线 */
    hr {
      margin: 3rem 0;
      background-color: #f1f1f1;
    }
    
    /* 表格标题 */
    .table-caption {
      font-size: 0.95rem; 
      color: #4a4a4a; 
      margin-top: 5px; 
      margin-bottom: 25px;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="font-size: 2.4rem; line-height: 1.2;">
            <span style="position: relative; display: inline-block;">
              <img src="./static/icon.png" 
                   alt="ROMA Icon" 
                   style="
                     position: absolute; 
                     right: 100%;       /* 放在文字容器的最左侧外 */
                     top: 50%;          /* 垂直居中 */
                     transform: translateY(-50%); 
                     width: 1.2em; 
                     margin-right: 15px; /* Logo 和文字的间距 */
                   ">
              ROMA: Real-time Omni-Multimodal Assistant
            </span>
            <br>
            with Interactive Streaming Understanding
          </h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="mailto:tianxueyun23z@ict.ac.cn">Xueyun Tian</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:weili.ucas.ict@gmail.com">Wei Li</a>,
            </span>
            <span class="author-block">
              <a href="mailto:xubingbing@ict.ac.cn">Bingbing Xu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:drdhxi@gmail.com">Heng Dong</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:wangyuanzhuo@ict.ac.cn">Yuanzhuo Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="mailto:shenhuawei@ict.ac.cn">Huawei Shen</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>CAS Key Laboratory of AI Safety, Institute of Computing Technology, CAS</span>
            <br>
            <span class="author-block"><sup>2</sup>University of Chinese Academy of Sciences</span>
            <span class="author-block"><sup>3</sup>Tsinghua University</span>
          </div>
          
          <div class="is-size-6 publication-authors" style="margin-top: 10px;">
            <span class="author-block"><small>tianxueyun23z@ict.ac.cn</small></span>
          </div>

          <div class="publication-links">
            <span class="link-block">
              <a href="#" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-light" style="pointer-events: none; opacity: 0.6;">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code (Coming Soon)</span>
              </a>
            </span>
            <span class="link-block">
              <a class="external-link button is-normal is-rounded is-light" style="pointer-events: none; opacity: 0.6;">
                <span class="icon"><i class="far fa-images"></i></span>
                <span>Dataset (Coming Soon)</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="./static/teaser.png" 
             alt="ROMA Streaming Capabilities" 
             class="result-image"
             style="max-height: 450px; width: auto;">
             
        <h2 class="subtitle has-text-justified teaser-caption">
          <strong>Figure 1: ROMA's streaming understanding capabilities.</strong> 
          It supports proactive tasks, including event alerts and narration, alongside reactive question answering.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified abstract-text">
            <p>
              Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring.
            </p>
            <p>
              To address this, we present ROMA, <strong>a real-time omni-multimodal assistant for unified reactive and proactive interaction.</strong> ROMA processes continuous inputs as synchronized <em>multimodal units</em>, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight <em>speak head</em> that decouples response initiation from generation to ensure precise triggering without task conflict.
            </p>
            <p>
              We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.
            </p>
          </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Demo Video</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/RzewWGTpM5I?rel=0&amp;showinfo=0"
        frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered">Methodology</h2>
        <div class="content has-text-justified">
          <p>
            ROMA unifies reactive answering and proactive timing over continuous inputs. The framework processes streaming signals as synchronized <strong>multimodal units</strong>, utilizing Chunked Time-aligned Multimodal RoPE (TMRoPE) for precise cross-modal alignment. By integrating a dedicated <strong>Speak Head</strong>, the model achieves robust temporal grounding and autonomous interaction control.
          </p>

          <div class="has-text-centered" style="margin: 2rem 0;">
            <img src="./static/architecture.png" 
                 alt="ROMA Architecture" 
                 class="result-image" 
                 style="width: 100%;">
            <p class="teaser-caption has-text-centered"><strong>Figure 2: Model Architecture.</strong> ROMA processes streaming inputs as aligned multimodal units. The speak head determines response timing, activating the LM head upon crossing a probability threshold.</p>
          </div>
          
          <p><strong>Key Mechanisms:</strong></p>
          <ul>
            <li>
              <strong>Aligned Multimodal Units & TMRoPE:</strong> 
              We segment continuous streams into fixed one-second intervals. Within each unit, dense audio signals and discrete video frames are synchronized via Chunked TMRoPE, which assigns cumulative positional IDs to maintain strict cross-modal correspondence along the global timeline.
            </li>
            <li>
              <strong>Decoupled Speak Head:</strong> 
              To enable proactive monitoring, we introduce a lightweight MLP module parallel to the LM head. This head explicitly predicts binary response probabilities based on the stream prefix, decoupling the decision of <em>"when to speak"</em> from <em>"what to generate"</em> to mitigate interference from generative biases.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">ROMA Streaming Dataset</h2>
    <div class="columns is-centered">
      <div class="column is-full-width">
         <div class="content has-text-centered">
          <p class="has-text-justified">
            We constructed a comprehensive streaming dataset covering Proactive (Alert, Narration) and Reactive (QA) tasks, totally over 676K samples.
          </p>
          <img src="./static/dataset.png" 
               alt="Dataset Statistics" 
               class="result-image" 
               style="width: 100%;">
          <p class="teaser-caption has-text-centered"><strong>Figure 4: Dataset Overview.</strong></p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Qualitative Analysis</h2>
    
    <div class="content">
      <h3 class="title is-4">Proactive Interaction: Event Alerts</h3>
      <p>ROMA can accurately detect short-duration events and recurrences in real-time streams.</p>
      
      <div class="columns is-centered">
        <div class="column is-full-width">
          <img src="./static/case_proactive.png" alt="Single Alert Case" class="result-image" style="width: 100%;">
          <p class="teaser-caption has-text-centered"><strong>Figure: Single Alert Case.</strong> ROMA triggers precise alerts for one-time events.</p>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <img src="./static/case_proactive_multi.png" alt="Multi Alert Case" class="result-image" style="width: 100%;">
          <p class="teaser-caption has-text-centered"><strong>Figure: Recurring Alert Case.</strong> ROMA handles recurring events effectively.</p>
        </div>
      </div>
    </div>

    <div class="content">
      <h3 class="title is-4">Proactive Interaction: Real-Time Narration</h3>
      <p>Compared with baseline VideoLLMs, ROMA provides more succinct and time-aligned summaries of events.</p>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <img src="./static/case_narration.png" alt="Narration Case Study" class="result-image" style="width: 100%;">
          <p class="teaser-caption has-text-centered"><strong>Figure: Narration Case Study.</strong> Comparison on YouCook2.</p>
        </div>
      </div>
    </div>

    <div class="content">
      <h3 class="title is-4">Reactive Interaction: Question Answering</h3>
      <p>In reactive QA settings, ROMA demonstrates robust <strong>omni-multimodal</strong> capabilities by directly processing <strong>audio queries</strong>, maintaining strong context understanding without relying on text transcription.</p>
      <div class="columns is-centered">
        <div class="column is-full-width">
          <img src="./static/case_turn.png" alt="Reactive QA Case" class="result-image" style="width: 100%;">
          <p class="teaser-caption has-text-centered"><strong>Figure: Reactive QA Case.</strong></p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Quantitative Results</h2>

    <h3 class="title is-4 results-subtitle">Proactive Tasks: Event-Driven Alert</h3>
    <div class="columns is-centered">
      <div class="column is-half has-text-centered">
        <img src="./static/alert1.png" alt="Alert Table 1" class="result-image" style="width: 100%;">
        <p class="table-caption"><strong>Table: Performance on QVHighlights & Charades-STA.</strong></p>
      </div>
      <div class="column is-half has-text-centered">
        <img src="./static/alert2.png" alt="Alert Table 2" class="result-image" style="width: 100%;">
        <p class="table-caption"><strong>Table: Single & Recurring Alert Performance.</strong></p>
      </div>
    </div>

    <h3 class="title is-4 results-subtitle">Proactive Tasks: Real-Time Narration</h3>
    <div class="columns is-centered">
      <div class="column is-10 has-text-centered">
        <img src="./static/narration.png" alt="Narration Table" class="result-image" style="width: 75%;">
        <p class="table-caption"><strong>Table: Streaming Narration on YouCook2 & OVO-Bench (SSR).</strong></p>
      </div>
    </div>

    <h3 class="title is-4 results-subtitle">Reactive Tasks: QA & Streaming Understanding</h3>
    
    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <img src="./static/ovo.png" alt="OVO Bench Table" class="result-image" style="width: 100%;">
        <p class="table-caption"><strong>Table: Reactive QA on OVO-Bench (Real-time Visual Perception & Backward Tracing).</strong></p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <img src="./static/streaming.png" alt="Streaming Bench Table" class="result-image" style="width: 100%;">
        <p class="table-caption"><strong>Table: Performance on StreamingBench.</strong></p>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-two-thirds has-text-centered">
        <img src="./static/omni.png" alt="Omni Bench Table" class="result-image" style="width: 100%;">
        <p class="table-caption"><strong>Table: Full-Modality QA on Video-MME & EgoSchema.</strong></p>
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre class="bibtex-box"><code>@article{roma2025,
  title={ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding},
  author={Anonymous Authors},
  journal={ACL Submission},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        Template adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
